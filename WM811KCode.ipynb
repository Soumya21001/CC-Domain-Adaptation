{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the WM811K dataset\n",
    "def load_wm811k_data(path_to_dataset):\n",
    "    # Load dataset and perform preprocessing based on provided script\n",
    "    df = pd.read_pickle(path_to_dataset)\n",
    "    df = df.drop(['waferIndex'], axis=1)  # Drop unnecessary columns\n",
    "    df['waferMapDim'] = df['waferMap'].apply(lambda x: (np.size(x, axis=0), np.size(x, axis=1)))\n",
    "    df['failureNum'] = df['failureType']\n",
    "    mapping_type = {'Center': 0, 'Donut': 1, 'Edge-Loc': 2, 'Edge-Ring': 3, 'Loc': 4, 'Random': 5, 'Scratch': 6, 'Near-full': 7, 'none': 8}\n",
    "    df = df.replace({'failureNum': mapping_type})\n",
    "\n",
    "    # Segregate into with pattern wafers only\n",
    "    df_withpattern = df[(df['failureNum'] >= 0) & (df['failureNum'] <= 7)].reset_index()\n",
    "\n",
    "    wafer_maps = df_withpattern['waferMap'].to_numpy()\n",
    "    labels = df_withpattern['failureNum'].to_numpy()\n",
    "\n",
    "    # Resize images to (32, 32) and normalize\n",
    "    data = []\n",
    "    for wafer_map in wafer_maps:\n",
    "        resized = np.array(wafer_map, dtype=np.float32)\n",
    "        if resized.shape[0] != 32 or resized.shape[1] != 32:\n",
    "            resized = np.resize(resized, (32, 32))\n",
    "        data.append(resized)\n",
    "\n",
    "    data = np.expand_dims(data, axis=1)  # Adding channel dimension\n",
    "    data = np.array(data)\n",
    "\n",
    "    return data, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_wm811k_data(\"C:/Users/Soumya Taneja/Desktop/Sideproject/WM811Kdata/LSWMD.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class for WM811K wafer map dataset\n",
    "def create_wafer_dataset(data, labels, transform=None):\n",
    "    return [(transform(image) if transform else image, label) for image, label in zip(data, labels)]\n",
    "\n",
    "# Split data into training and test sets\n",
    "data, labels = load_wm811k_data(\"C:/Users/Soumya Taneja/Desktop/Sideproject/WM811Kdata/LSWMD.pkl\")\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Data augmentation and normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = create_wafer_dataset(train_data, train_labels, transform=transform)\n",
    "test_dataset = create_wafer_dataset(test_data, test_labels, transform=transform)\n",
    "\n",
    "# DataLoader for batch processing\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature generator (VGG16) and classifiers\n",
    "feature_generator = models.vgg16(pretrained=False)  # VGG16 without pretrained weights\n",
    "feature_generator.features[0] = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)  # Adjusting input channel\n",
    "feature_generator.classifier[6] = nn.Linear(4096, 256)\n",
    "\n",
    "classifier_main = nn.Linear(256, len(np.unique(labels)))\n",
    "classifier_aux1 = nn.Linear(256, len(np.unique(labels)))\n",
    "classifier_aux2 = nn.Linear(256, len(np.unique(labels)))\n",
    "\n",
    "# Model, loss, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "feature_generator = feature_generator.to(device)\n",
    "classifier_main = classifier_main.to(device)\n",
    "classifier_aux1 = classifier_aux1.to(device)\n",
    "classifier_aux2 = classifier_aux2.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(list(feature_generator.parameters()) + list(classifier_main.parameters()) +\n",
    "                       list(classifier_aux1.parameters()) + list(classifier_aux2.parameters()),\n",
    "                       lr=2e-4, betas=(0.9, 0.999), eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    feature_generator.train()\n",
    "    classifier_main.train()\n",
    "    classifier_aux1.train()\n",
    "    classifier_aux2.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Adjust the input shape to match expected input (batch_size, channels, height, width)\n",
    "        images = images.view(-1, 1, 32, 32)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        features = feature_generator(images)\n",
    "        outputs_main = classifier_main(features)\n",
    "        outputs_aux1 = classifier_aux1(features)\n",
    "        outputs_aux2 = classifier_aux2(features)\n",
    "\n",
    "        # Loss calculation for main and auxiliary classifiers\n",
    "        loss_main = criterion(outputs_main, labels)\n",
    "        loss_aux1 = criterion(outputs_aux1, labels)\n",
    "        loss_aux2 = criterion(outputs_aux2, labels)\n",
    "        loss = loss_main + 0.4 * (loss_aux1 + loss_aux2) #Can experiment with differenet values\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loop\n",
    "feature_generator.eval()\n",
    "classifier_main.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images = images.view(-1, 1, 32, 32)  # Adjust the input shape to match expected input (batch_size, channels, height, width)\n",
    "        features = feature_generator(images)\n",
    "        outputs = classifier_main(features)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some test results\n",
    "fig, axes = plt.subplots(1, 10, figsize=(20, 10))\n",
    "with torch.no_grad():\n",
    "    for i in range(10):\n",
    "        img, label = test_dataset[i]\n",
    "        img_tensor = img.unsqueeze(0).to(device)\n",
    "        img_tensor = img_tensor.view(1, 1, 32, 32)\n",
    "        features = feature_generator(img_tensor)\n",
    "        output = classifier_main(features)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "\n",
    "        img = img.squeeze().cpu().numpy()\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(f\"True: {label}, Pred: {predicted.item()}\")\n",
    "        axes[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2nd version\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess the WM811K dataset\n",
    "def load_wm811k_data(path_to_dataset):\n",
    "    # Load dataset and perform preprocessing based on provided script\n",
    "    df = pd.read_pickle(path_to_dataset)\n",
    "    df = df.drop(['waferIndex'], axis=1)  # Drop unnecessary columns\n",
    "    df['waferMapDim'] = df['waferMap'].apply(lambda x: (np.size(x, axis=0), np.size(x, axis=1)))\n",
    "    df['failureNum'] = df['failureType']\n",
    "    mapping_type = {'Center': 0, 'Donut': 1, 'Edge-Loc': 2, 'Edge-Ring': 3, 'Loc': 4, 'Random': 5, 'Scratch': 6, 'Near-full': 7, 'none': 8}\n",
    "    df = df.replace({'failureNum': mapping_type})\n",
    "\n",
    "    # Segregate into with pattern wafers only\n",
    "    df_withpattern = df[(df['failureNum'] >= 0) & (df['failureNum'] <= 7)].reset_index()\n",
    "\n",
    "    wafer_maps = df_withpattern['waferMap'].to_numpy()\n",
    "    labels = df_withpattern['failureNum'].to_numpy()\n",
    "\n",
    "    # Resize images to (32, 32) and normalize\n",
    "    data = []\n",
    "    for wafer_map in wafer_maps:\n",
    "        resized = np.array(wafer_map, dtype=np.float32)\n",
    "        if resized.shape[0] != 32 or resized.shape[1] != 32:\n",
    "            resized = np.resize(resized, (32, 32))\n",
    "        data.append(resized)\n",
    "\n",
    "    data = np.expand_dims(data, axis=1)  # Adding channel dimension\n",
    "    data = np.array(data)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Custom Dataset class for WM811K wafer map dataset\n",
    "def create_wafer_dataset(data, labels, transform=None):\n",
    "    return [(transform(image) if transform else image, label) for image, label in zip(data, labels)]\n",
    "\n",
    "# Split data into source (labeled) and target (unlabeled) domains\n",
    "data, labels = load_wm811k_data(\"C:/Users/Soumya Taneja/Desktop/Sideproject/WM811Kdata/LSWMD.pkl\")\n",
    "train_data_source, train_data_target, train_labels_source, _ = train_test_split(data, labels, test_size=0.5, random_state=42)\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(train_data_source, train_labels_source, test_size=0.2, random_state=42)\n",
    "\n",
    "# Data augmentation and normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = create_wafer_dataset(train_data, train_labels, transform=transform)\n",
    "test_dataset = create_wafer_dataset(test_data, test_labels, transform=transform)\n",
    "target_dataset = create_wafer_dataset(train_data_target, [0] * len(train_data_target), transform=transform)  # Target domain without labels\n",
    "\n",
    "# DataLoader for batch processing\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "target_loader = DataLoader(target_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the feature generator (VGG16) and classifiers\n",
    "feature_generator = models.vgg16(pretrained=False)  # VGG16 without pretrained weights\n",
    "feature_generator.features[0] = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)  # Adjusting input channel\n",
    "feature_generator.classifier[6] = nn.Linear(4096, 256)\n",
    "\n",
    "classifier_main = nn.Linear(256, len(np.unique(labels)))\n",
    "classifier_aux1 = nn.Linear(256, len(np.unique(labels)))\n",
    "classifier_aux2 = nn.Linear(256, len(np.unique(labels)))\n",
    "\n",
    "# Model, loss, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "feature_generator = feature_generator.to(device)\n",
    "classifier_main = classifier_main.to(device)\n",
    "classifier_aux1 = classifier_aux1.to(device)\n",
    "classifier_aux2 = classifier_aux2.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(list(feature_generator.parameters()) + list(classifier_main.parameters()) +\n",
    "                       list(classifier_aux1.parameters()) + list(classifier_aux2.parameters()),\n",
    "                       lr=2e-4, betas=(0.9, 0.999), eps=1e-8)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    feature_generator.train()\n",
    "    classifier_main.train()\n",
    "    classifier_aux1.train()\n",
    "    classifier_aux2.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Train on source domain (labeled data)\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Adjust the input shape to match expected input (batch_size, channels, height, width)\n",
    "        images = images.view(-1, 1, 32, 32)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        features = feature_generator(images)\n",
    "        outputs_main = classifier_main(features)\n",
    "        outputs_aux1 = classifier_aux1(features)\n",
    "        outputs_aux2 = classifier_aux2(features)\n",
    "\n",
    "        # Loss calculation for main and auxiliary classifiers\n",
    "        loss_main = criterion(outputs_main, labels)\n",
    "        loss_aux1 = criterion(outputs_aux1, labels)\n",
    "        loss_aux2 = criterion(outputs_aux2, labels)\n",
    "        loss = loss_main + 0.3 * (loss_aux1 + loss_aux2)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Print detailed outputs for every 100 batches\n",
    "        #if (i + 1) % 100 == 0:\n",
    "            #print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Train on target domain (unlabeled data) for feature alignment\n",
    "    for i, (images, _) in enumerate(target_loader):\n",
    "        images = images.to(device)\n",
    "        images = images.view(-1, 1, 32, 32)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        features = feature_generator(images)\n",
    "        outputs_aux1 = classifier_aux1(features)\n",
    "        outputs_aux2 = classifier_aux2(features)\n",
    "\n",
    "        # Adversarial loss to align target features with source features\n",
    "        soft_labels = torch.softmax(outputs_aux1, dim=1)  # Generate pseudo-labels from auxiliary classifier\n",
    "        loss_aux = criterion(outputs_aux2, soft_labels.argmax(dim=1))\n",
    "        loss = 0.3 * loss_aux  # Weight the auxiliary loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Testing loop\n",
    "feature_generator.eval()\n",
    "classifier_main.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images = images.view(-1, 1, 32, 32)  # Adjust the input shape to match expected input (batch_size, channels, height, width)\n",
    "        features = feature_generator(images)\n",
    "        outputs = classifier_main(features)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Visualize some test results\n",
    "fig, axes = plt.subplots(1, 10, figsize=(15, 5))\n",
    "with torch.no_grad():\n",
    "    for i in range(10):\n",
    "        img, label = test_dataset[i]\n",
    "        img_tensor = img.unsqueeze(0).to(device)\n",
    "        img_tensor = img_tensor.view(1, 1, 32, 32)\n",
    "        features = feature_generator(img_tensor)\n",
    "        output = classifier_main(features)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "\n",
    "        img = img.squeeze().cpu().numpy()\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(f\"True: {label}, Pred: {predicted.item()}\")\n",
    "        axes[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Soumya Taneja\\myAnaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Soumya Taneja\\myAnaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Source Loss: 2.5061, Target Loss: 0.0071\n",
      "Epoch [2/50], Source Loss: 3.5530, Target Loss: 0.0059\n",
      "Epoch [3/50], Source Loss: 3.1645, Target Loss: 0.0076\n",
      "Epoch [4/50], Source Loss: 3.2052, Target Loss: 0.0027\n",
      "Epoch [5/50], Source Loss: 2.5831, Target Loss: 0.0069\n",
      "Epoch [6/50], Source Loss: 2.8499, Target Loss: 0.0053\n",
      "Epoch [7/50], Source Loss: 4.7034, Target Loss: 0.0088\n",
      "Epoch [8/50], Source Loss: 3.0919, Target Loss: 0.0081\n",
      "Epoch [9/50], Source Loss: 2.7636, Target Loss: 0.0042\n",
      "Epoch [10/50], Source Loss: 2.6702, Target Loss: 0.0047\n",
      "Epoch [11/50], Source Loss: 2.9670, Target Loss: 0.0069\n",
      "Epoch [12/50], Source Loss: 2.9634, Target Loss: 0.0060\n",
      "Epoch [13/50], Source Loss: 5.0783, Target Loss: 0.0081\n",
      "Epoch [14/50], Source Loss: 2.8851, Target Loss: 0.0134\n",
      "Epoch [15/50], Source Loss: 8.2109, Target Loss: 0.0081\n",
      "Epoch [16/50], Source Loss: 4.1226, Target Loss: 0.0053\n",
      "Epoch [17/50], Source Loss: 2.4289, Target Loss: 0.0156\n",
      "Epoch [18/50], Source Loss: 3.8798, Target Loss: 0.0143\n",
      "Epoch [19/50], Source Loss: 3.0527, Target Loss: 0.0036\n",
      "Epoch [20/50], Source Loss: 3.4117, Target Loss: 0.0016\n",
      "Epoch [21/50], Source Loss: 2.5238, Target Loss: 0.0135\n",
      "Epoch [22/50], Source Loss: 4.0079, Target Loss: 0.0093\n",
      "Epoch [23/50], Source Loss: 2.7120, Target Loss: 0.0195\n",
      "Epoch [24/50], Source Loss: 2.8046, Target Loss: 0.0090\n",
      "Epoch [25/50], Source Loss: 4.9936, Target Loss: 0.0382\n",
      "Epoch [26/50], Source Loss: 6.4793, Target Loss: 0.0210\n",
      "Epoch [27/50], Source Loss: 3.1009, Target Loss: 0.0326\n",
      "Epoch [28/50], Source Loss: 3.0094, Target Loss: 0.0147\n",
      "Epoch [29/50], Source Loss: 3.0348, Target Loss: 0.0041\n",
      "Epoch [30/50], Source Loss: 2.4276, Target Loss: 0.0211\n",
      "Epoch [31/50], Source Loss: 2.3567, Target Loss: 0.0243\n",
      "Epoch [32/50], Source Loss: 1.9436, Target Loss: 0.0515\n",
      "Epoch [33/50], Source Loss: 1.8675, Target Loss: 0.0303\n",
      "Epoch [34/50], Source Loss: 1.8818, Target Loss: 0.0250\n",
      "Epoch [35/50], Source Loss: 2.0678, Target Loss: 0.0098\n",
      "Epoch [36/50], Source Loss: 4.2811, Target Loss: 0.0218\n",
      "Epoch [37/50], Source Loss: 10.9015, Target Loss: 0.0058\n",
      "Epoch [38/50], Source Loss: 2.8033, Target Loss: 0.0207\n",
      "Epoch [39/50], Source Loss: 2.7917, Target Loss: 0.0050\n",
      "Epoch [40/50], Source Loss: 2.9638, Target Loss: 0.0059\n",
      "Epoch [41/50], Source Loss: 2.1463, Target Loss: 0.0160\n",
      "Epoch [42/50], Source Loss: 1.7877, Target Loss: 0.0434\n",
      "Epoch [43/50], Source Loss: 1.5761, Target Loss: 0.0134\n",
      "Epoch [44/50], Source Loss: 25.3714, Target Loss: 0.0093\n",
      "Epoch [45/50], Source Loss: 2.0828, Target Loss: 0.0079\n",
      "Epoch [46/50], Source Loss: 2.4961, Target Loss: 0.0174\n",
      "Epoch [47/50], Source Loss: 2.3449, Target Loss: 0.0220\n",
      "Epoch [48/50], Source Loss: 3.2642, Target Loss: 0.0267\n",
      "Epoch [49/50], Source Loss: 2.2862, Target Loss: 0.0229\n",
      "Epoch [50/50], Source Loss: 2.0011, Target Loss: 0.0325\n",
      "Test Accuracy: 44.28%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#3rd version\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess the WM811K dataset\n",
    "def load_wm811k_data(path_to_dataset):\n",
    "    # Load dataset and perform preprocessing based on provided script\n",
    "    df = pd.read_pickle(path_to_dataset)\n",
    "    df = df.drop(['waferIndex'], axis=1)  # Drop unnecessary columns\n",
    "    df['waferMapDim'] = df['waferMap'].apply(lambda x: (np.size(x, axis=0), np.size(x, axis=1)))\n",
    "    df['failureNum'] = df['failureType']\n",
    "    mapping_type = {'Center': 0, 'Donut': 1, 'Edge-Loc': 2, 'Edge-Ring': 3, 'Loc': 4, 'Random': 5, 'Scratch': 6, 'Near-full': 7, 'none': 8}\n",
    "    df = df.replace({'failureNum': mapping_type})\n",
    "\n",
    "    # Segregate into with pattern wafers only\n",
    "    df_withpattern = df[(df['failureNum'] >= 0) & (df['failureNum'] <= 7)].reset_index()\n",
    "\n",
    "    wafer_maps = df_withpattern['waferMap'].to_numpy()\n",
    "    labels = df_withpattern['failureNum'].to_numpy()\n",
    "\n",
    "    # Resize images to (32, 32) and normalize\n",
    "    data = []\n",
    "    for wafer_map in wafer_maps:\n",
    "        resized = np.array(wafer_map, dtype=np.float32)\n",
    "        if resized.shape[0] != 32 or resized.shape[1] != 32:\n",
    "            resized = np.resize(resized, (32, 32))\n",
    "        data.append(resized)\n",
    "\n",
    "    data = np.expand_dims(data, axis=1)  # Adding channel dimension\n",
    "    data = np.array(data)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Custom Dataset class for WM811K wafer map dataset\n",
    "def create_wafer_dataset(data, labels, transform=None):\n",
    "    return [(transform(image) if transform else image, label) for image, label in zip(data, labels)]\n",
    "\n",
    "# Split data into source (labeled) and target (unlabeled) domains\n",
    "data, labels = load_wm811k_data(\"C:/Users/Soumya Taneja/Desktop/Sideproject/WM811Kdata/LSWMD.pkl\")\n",
    "train_data_source, train_data_target, train_labels_source, _ = train_test_split(data, labels, test_size=0.5, random_state=42)\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(train_data_source, train_labels_source, test_size=0.2, random_state=42)\n",
    "\n",
    "# Data augmentation and normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = create_wafer_dataset(train_data, train_labels, transform=transform)\n",
    "test_dataset = create_wafer_dataset(test_data, test_labels, transform=transform)\n",
    "target_dataset = create_wafer_dataset(train_data_target, [0] * len(train_data_target), transform=transform)  # Target domain without labels\n",
    "\n",
    "# DataLoader for batch processing\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "target_loader = DataLoader(target_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the feature generator (VGG16) and classifiers\n",
    "feature_generator = models.vgg16(pretrained=False)  # VGG16 without pretrained weights\n",
    "feature_generator.features[0] = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)  # Adjusting input channel\n",
    "feature_generator.classifier[6] = nn.Linear(4096, 256)\n",
    "\n",
    "classifier_main = nn.Linear(256, len(np.unique(labels)))\n",
    "classifier_aux1 = nn.Linear(256, len(np.unique(labels)))\n",
    "classifier_aux2 = nn.Linear(256, len(np.unique(labels)))\n",
    "\n",
    "# Model, loss, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "feature_generator = feature_generator.to(device)\n",
    "classifier_main = classifier_main.to(device)\n",
    "classifier_aux1 = classifier_aux1.to(device)\n",
    "classifier_aux2 = classifier_aux2.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(list(feature_generator.parameters()) + list(classifier_main.parameters()) +\n",
    "                       list(classifier_aux1.parameters()) + list(classifier_aux2.parameters()),\n",
    "                       lr=2e-4, betas=(0.9, 0.999), eps=1e-8)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    feature_generator.train()\n",
    "    classifier_main.train()\n",
    "    classifier_aux1.train()\n",
    "    classifier_aux2.train()\n",
    "    running_loss_source = 0.0\n",
    "    running_loss_target = 0.0\n",
    "\n",
    "    # Train on source domain (labeled data)\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Adjust the input shape to match expected input (batch_size, channels, height, width)\n",
    "        images = images.view(-1, 1, 32, 32)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        features = feature_generator(images)\n",
    "        outputs_main = classifier_main(features)\n",
    "        outputs_aux1 = classifier_aux1(features)\n",
    "        outputs_aux2 = classifier_aux2(features)\n",
    "\n",
    "        # Loss calculation for main and auxiliary classifiers\n",
    "        loss_main = criterion(outputs_main, labels)\n",
    "        loss_aux1 = criterion(outputs_aux1, labels)\n",
    "        loss_aux2 = criterion(outputs_aux2, labels)\n",
    "        loss_source = loss_main + 0.5 * (loss_aux1 + loss_aux2)\n",
    "\n",
    "        # Backward pass and optimization for source domain\n",
    "        loss_source.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss_source += loss_source.item()\n",
    "\n",
    "        # Print detailed outputs for every 100 batches\n",
    "        #if (i + 1) % 100 == 0:\n",
    "            #print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Source Loss: {loss_source.item():.4f}\")\n",
    "\n",
    "    # Train on target domain (unlabeled data) for feature alignment\n",
    "    for i, (images, _) in enumerate(target_loader):\n",
    "        images = images.to(device)\n",
    "        images = images.view(-1, 1, 32, 32)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        features = feature_generator(images)\n",
    "        outputs_aux1 = classifier_aux1(features)\n",
    "        outputs_aux2 = classifier_aux2(features)\n",
    "\n",
    "        # Adversarial loss to align target features with source features\n",
    "        soft_labels = torch.softmax(outputs_aux1, dim=1)  # Generate pseudo-labels from auxiliary classifier\n",
    "        loss_target = criterion(outputs_aux2, soft_labels.argmax(dim=1))\n",
    "        loss_target = 0.3 * loss_target  # Weight the auxiliary loss\n",
    "\n",
    "        # Backward pass and optimization for target domain\n",
    "        loss_target.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss_target += loss_target.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Source Loss: {running_loss_source / len(train_loader):.4f}, Target Loss: {running_loss_target / len(target_loader):.4f}\")\n",
    "\n",
    "# Testing loop\n",
    "feature_generator.eval()\n",
    "classifier_main.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images = images.view(-1, 1, 32, 32)  # Adjust the input shape to match expected input (batch_size, channels, height, width)\n",
    "        features = feature_generator(images)\n",
    "        outputs = classifier_main(features)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Visualize some test results\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        img, label = test_dataset[i]\n",
    "        img_tensor = img.unsqueeze(0).to(device)\n",
    "        img_tensor = img_tensor.view(1, 1, 32, 32)\n",
    "        features = feature_generator(img_tensor)\n",
    "        output = classifier_main(features)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "\n",
    "        img = img.squeeze().cpu().numpy()\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(f\"True: {label}, Pred: {predicted.item()}\")\n",
    "        axes[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
