{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch images shape: torch.Size([32, 1, 32, 32])\n",
      "Batch labels shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Preprocessing and Loading the WM811K Dataset\n",
    "def load_wm811k_data(path_to_dataset):\n",
    "    \"\"\"\n",
    "    Load and preprocess the WM811K dataset.\n",
    "\n",
    "    Parameters:\n",
    "        path_to_dataset (str): Path to the WM811K .pkl dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Preprocessed data and labels.\n",
    "    \"\"\"\n",
    "    # Load dataset and drop unnecessary columns\n",
    "    df = pd.read_pickle(path_to_dataset)\n",
    "    df = df.drop(['waferIndex'], axis=1)\n",
    "    df['waferMapDim'] = df['waferMap'].apply(lambda x: (np.size(x, axis=0), np.size(x, axis=1)))\n",
    "    df['failureNum'] = df['failureType']\n",
    "\n",
    "    # Map failure types to numerical values\n",
    "    mapping_type = {\n",
    "        'Center': 0, 'Donut': 1, 'Edge-Loc': 2, 'Edge-Ring': 3,\n",
    "        'Loc': 4, 'Random': 5, 'Scratch': 6, 'Near-full': 7, 'none': 8\n",
    "    }\n",
    "    df = df.replace({'failureNum': mapping_type})\n",
    "\n",
    "    # Select wafers with valid patterns\n",
    "    df_withpattern = df[(df['failureNum'] >= 0) & (df['failureNum'] <= 7)].reset_index()\n",
    "\n",
    "    wafer_maps = df_withpattern['waferMap'].to_numpy()\n",
    "    labels = df_withpattern['failureNum'].to_numpy()\n",
    "\n",
    "    # Resize images to (32, 32) and normalize\n",
    "    data = []\n",
    "    for wafer_map in wafer_maps:\n",
    "        resized = np.array(wafer_map, dtype=np.float32)\n",
    "        if resized.shape[0] != 32 or resized.shape[1] != 32:\n",
    "            resized = np.resize(resized, (32, 32))\n",
    "        data.append(resized)\n",
    "\n",
    "    data = np.expand_dims(data, axis=1)  # Adding channel dimension for PyTorch\n",
    "    data = np.array(data)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Custom Dataset for PyTorch\n",
    "class WaferMapDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Custom dataset for wafer maps.\n",
    "\n",
    "        Parameters:\n",
    "            data (np.ndarray): Preprocessed wafer map images.\n",
    "            labels (np.ndarray): Corresponding labels.\n",
    "            transform (callable, optional): Transformations for data augmentation.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return torch.tensor(image, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Path to WM811K dataset\n",
    "path_to_dataset = \"C:/Users/Soumya Taneja/Desktop/Sideproject/WM811Kdata/LSWMD.pkl\"\n",
    "\n",
    "# Load and preprocess the data\n",
    "data, labels = load_wm811k_data(path_to_dataset)\n",
    "\n",
    "# Define data transformations (if needed)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalization for grayscale images\n",
    "])\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "dataset = WaferMapDataset(data, labels, transform=None)  # Apply transforms if needed\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Example: Check a batch of data\n",
    "for batch_images, batch_labels in dataloader:\n",
    "    print(\"Batch images shape:\", batch_images.shape)\n",
    "    print(\"Batch labels shape:\", batch_labels.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16Classifier(nn.Module):\n",
    "    def __init__(self, init_weights=True, num_class=10,  feature_dim = 512):\n",
    "        self.num_class = num_class\n",
    "        super(VGG16Classifier, self).__init__()\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(feature_dim, 4096),\n",
    "            nn.BatchNorm1d(4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.BatchNorm1d(4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_class),\n",
    "        )\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        loss = 0\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "                \n",
    "def make_variable(tensor):\n",
    "    if torch.cuda.is_available():\n",
    "        tensor = tensor.to('cuda')\n",
    "    return Variable(tensor)\n",
    "\n",
    "\n",
    "def load_training(root_path, dir, batch_size, kwargs):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    data = datasets.ImageFolder(root=root_path + dir, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True, **kwargs)\n",
    "    return train_loader\n",
    "\n",
    "def load_testing(root_path, dir, batch_size, kwargs):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    data = datasets.ImageFolder(root=root_path + dir, transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    return test_loader\n",
    "\n",
    "def load_datasets(root_path, src_dataset, tgt_dataset, batch_size):\n",
    "    no_cuda = False\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5\"\n",
    "    cuda = not no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(8)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed(8)\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "    src_data_loader = load_training(root_path, src_dataset, batch_size, kwargs)\n",
    "    tgt_data_loader = load_training(root_path, tgt_dataset, batch_size, kwargs)\n",
    "    tgt_data_test = load_testing(root_path, tgt_dataset, batch_size, kwargs)\n",
    "    return src_data_loader, tgt_data_loader, tgt_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A high discrepancy (large loss_adv) indicates that the two classifiers disagree on the predictions, suggesting poor alignment between the source and target domains.\\nA low discrepancy (small loss_adv) means better alignment between the source and target domains.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CDADA(object):\n",
    "\n",
    "    def __init__(self, pkl_file_path, num_class=8, max_epoch=100, batch_size=32, learning_rate=0.0001 ):\n",
    "        self.pkl_file_path = pkl_file_path\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = learning_rate\n",
    "        self.t_correct = 0\n",
    "        self.updata = 4\n",
    "\n",
    "        # Load the dataset from the .pkl file\n",
    "        data, labels = load_wm811k_data(self.pkl_file_path)\n",
    "\n",
    "        # Split into training and testing datasets\n",
    "        split_ratio = 0.8\n",
    "        split_index = int(len(data) * split_ratio)\n",
    "        src_data, tgt_data = data[:split_index], data[split_index:]\n",
    "        src_labels, tgt_labels = labels[:split_index], labels[split_index:]\n",
    "\n",
    "        self.datasets_source = DataLoader(\n",
    "            WaferMapDataset(src_data, src_labels),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "        self.dataset_target = DataLoader(\n",
    "            WaferMapDataset(tgt_data, tgt_labels),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "        self.dataset_target_test = DataLoader(\n",
    "            WaferMapDataset(tgt_data, tgt_labels),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        # Load VGG16 model\n",
    "        model = models.vgg16(pretrained=False)\n",
    "        model.load_state_dict(torch.load('C:/Users/Soumya Taneja/Desktop/Sideproject/vgg16-397923af.pth'))\n",
    "\n",
    "        # Modify the first convolutional layer for 1-channel input\n",
    "        model.features[0] = nn.Conv2d(\n",
    "            in_channels=1,  # Change from 3 to 1\n",
    "            out_channels=64,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        \n",
    "        for i, para in enumerate(model.features.parameters()):\n",
    "            if i < 24:\n",
    "                para.requires_grad = False\n",
    "\n",
    "        \n",
    "        self.Generator = model.features\n",
    "        self.Generator.to('cuda')  # Move the model to GPU\n",
    "\n",
    "        # Debugging: Check the shape of the output from Generator\n",
    "        sample_input = torch.randn(1, 1, 32, 32).to('cuda')  # Example input (batch size 1, 1 channel, 32x32)\n",
    "        sample_output = self.Generator(sample_input)\n",
    "        print(\"Shape of Generator output:\", sample_output.shape)\n",
    "        # Determine feature_dim dynamically\n",
    "        feature_dim = 512\n",
    "        self.Classifier = VGG16Classifier(num_class=num_class, feature_dim =feature_dim)\n",
    "        self.Classifier1 = VGG16Classifier(num_class=num_class, feature_dim =feature_dim)\n",
    "        self.Classifier2 = VGG16Classifier(num_class=num_class, feature_dim =feature_dim)\n",
    "       \n",
    "        self.Generator.to('cuda')\n",
    "        self.Classifier.to('cuda')\n",
    "        self.Classifier1.to('cuda')\n",
    "        self.Classifier2.to('cuda')\n",
    "\n",
    "        self.opt_generator = optim.Adam(filter(lambda p: p.requires_grad, self.Generator.parameters()),\n",
    "                                        lr=self.lr, weight_decay=0.0005)\n",
    "        self.opt_classifier = optim.Adam(self.Classifier.parameters(),\n",
    "                                         lr=self.lr, weight_decay=0.0005)\n",
    "        self.opt_classifier1 = optim.Adam(self.Classifier1.parameters(),\n",
    "                                          lr=self.lr, weight_decay=0.0005)\n",
    "        self.opt_classifier2 = optim.Adam(self.Classifier2.parameters(),\n",
    "                                          lr=self.lr, weight_decay=0.0005)\n",
    "\n",
    "    def reset_grad(self):\n",
    "        self.opt_generator.zero_grad()\n",
    "        self.opt_classifier.zero_grad()\n",
    "        self.opt_classifier1.zero_grad()\n",
    "        self.opt_classifier2.zero_grad()\n",
    "\n",
    "    def test(self):\n",
    "        self.Generator.eval()\n",
    "        self.Classifier.eval()\n",
    "        correct = 0\n",
    "        size = 0\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            for data, target in self.dataset_target_test:\n",
    "                img = make_variable(data).to('cuda')\n",
    "                label = make_variable(target).to('cuda')\n",
    "\n",
    "                feat = self.Generator(img)\n",
    "                pred = self.Classifier(feat)\n",
    "\n",
    "                pred = pred.argmax(dim=1)  #pred = pred.data.max(1)[1]\n",
    "                k = label.data.size()[0]\n",
    "                correct += pred.eq(label).sum().item()\n",
    "                size += k\n",
    "\n",
    "        if correct > self.t_correct:\n",
    "            self.t_correct = correct\n",
    "\n",
    "        print('Accuracy: {}/{} ({:.2f}%) Max Accuracy: {}/{} ({:.2f}%) \\n'.\n",
    "              format(correct, size, 100. * correct / size, self.t_correct, size, 100. * self.t_correct / size))\n",
    "        \n",
    "    def train(self):\n",
    "        criterion = nn.CrossEntropyLoss().to('cuda')\n",
    "        self.Generator.train()\n",
    "        self.Classifier.train()\n",
    "        self.Classifier1.train()\n",
    "        self.Classifier2.train()\n",
    "        torch.cuda.manual_seed(1)\n",
    "\n",
    "        for ep in range(self.max_epoch):\n",
    "            data_zip = enumerate(zip(self.datasets_source, self.dataset_target))\n",
    "            for step, ((images_src, label), (images_tgt, _)) in data_zip:\n",
    "                img_src = make_variable(images_src).to('cuda')\n",
    "                label_src = make_variable(label.squeeze_()).to('cuda')\n",
    "                img_tgt = make_variable(images_tgt).to('cuda')\n",
    "\n",
    "                self.reset_grad()\n",
    "                feat_src = self.Generator(img_src)\n",
    "                pred_src_c = self.Classifier(feat_src)\n",
    "                pred_src_c1 = self.Classifier1(feat_src)\n",
    "                pred_src_c2 = self.Classifier2(feat_src)\n",
    "\n",
    "                loss_src_c = criterion(pred_src_c, label_src)\n",
    "                loss_src_c1 = criterion(pred_src_c1, label_src)\n",
    "                loss_src_c2 = criterion(pred_src_c2, label_src)\n",
    "                loss_src = loss_src_c + loss_src_c1 + loss_src_c2\n",
    "\n",
    "                loss_src.backward()\n",
    "                self.opt_generator.step()\n",
    "                self.opt_classifier.step()\n",
    "                self.opt_classifier1.step()\n",
    "                self.opt_classifier2.step()\n",
    "\n",
    "                self.reset_grad()\n",
    "                feat_tgt = self.Generator(img_tgt)\n",
    "                pred_tgt_c1 = self.Classifier1(feat_tgt)\n",
    "                pred_tgt_c2 = self.Classifier2(feat_tgt)\n",
    "                p1 = F.softmax(pred_tgt_c1, dim=1)\n",
    "                p2 = F.softmax(pred_tgt_c2, dim=1)\n",
    "                loss_adv = torch.mean(torch.abs(p1 - p2))\n",
    "                loss_adv.backward()\n",
    "                self.opt_generator.step()\n",
    "                \n",
    "\n",
    "            print('Train Epoch:{} Adversarial Loss: {:.6f}'.format(ep+1, loss_adv.item())) #The adversarial loss is the mean absolute difference between the two classifiers' probability outputs.\n",
    "            self.test()\n",
    "                \n",
    "\n",
    "'''A high discrepancy (large loss_adv) indicates that the two classifiers disagree on the predictions, suggesting poor alignment between the source and target domains.\n",
    "A low discrepancy (small loss_adv) means better alignment between the source and target domains.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Soumya Taneja\\myAnaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Soumya Taneja\\myAnaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Generator output: torch.Size([1, 512, 1, 1])\n",
      "Train Epoch:1 Adversarial Loss: 0.090162\n",
      "Accuracy: 907/5104 (17.77%) Max Accuracy: 907/5104 (17.77%) \n",
      "\n",
      "Train Epoch:2 Adversarial Loss: 0.085056\n",
      "Accuracy: 1628/5104 (31.90%) Max Accuracy: 1628/5104 (31.90%) \n",
      "\n",
      "Train Epoch:3 Adversarial Loss: 0.108401\n",
      "Accuracy: 2210/5104 (43.30%) Max Accuracy: 2210/5104 (43.30%) \n",
      "\n",
      "Train Epoch:4 Adversarial Loss: 0.085129\n",
      "Accuracy: 2159/5104 (42.30%) Max Accuracy: 2210/5104 (43.30%) \n",
      "\n",
      "Train Epoch:5 Adversarial Loss: 0.073343\n",
      "Accuracy: 2130/5104 (41.73%) Max Accuracy: 2210/5104 (43.30%) \n",
      "\n",
      "Train Epoch:6 Adversarial Loss: 0.075414\n",
      "Accuracy: 1947/5104 (38.15%) Max Accuracy: 2210/5104 (43.30%) \n",
      "\n",
      "Train Epoch:7 Adversarial Loss: 0.059267\n",
      "Accuracy: 2164/5104 (42.40%) Max Accuracy: 2210/5104 (43.30%) \n",
      "\n",
      "Train Epoch:8 Adversarial Loss: 0.067371\n",
      "Accuracy: 1843/5104 (36.11%) Max Accuracy: 2210/5104 (43.30%) \n",
      "\n",
      "Train Epoch:9 Adversarial Loss: 0.056067\n",
      "Accuracy: 2067/5104 (40.50%) Max Accuracy: 2210/5104 (43.30%) \n",
      "\n",
      "Train Epoch:10 Adversarial Loss: 0.045266\n",
      "Accuracy: 1988/5104 (38.95%) Max Accuracy: 2210/5104 (43.30%) \n",
      "\n",
      "Train Epoch:11 Adversarial Loss: 0.069903\n",
      "Accuracy: 2243/5104 (43.95%) Max Accuracy: 2243/5104 (43.95%) \n",
      "\n",
      "Train Epoch:12 Adversarial Loss: 0.052563\n",
      "Accuracy: 2104/5104 (41.22%) Max Accuracy: 2243/5104 (43.95%) \n",
      "\n",
      "Train Epoch:13 Adversarial Loss: 0.070723\n",
      "Accuracy: 2227/5104 (43.63%) Max Accuracy: 2243/5104 (43.95%) \n",
      "\n",
      "Train Epoch:14 Adversarial Loss: 0.060733\n",
      "Accuracy: 2118/5104 (41.50%) Max Accuracy: 2243/5104 (43.95%) \n",
      "\n",
      "Train Epoch:15 Adversarial Loss: 0.051364\n",
      "Accuracy: 1801/5104 (35.29%) Max Accuracy: 2243/5104 (43.95%) \n",
      "\n",
      "Train Epoch:16 Adversarial Loss: 0.047399\n",
      "Accuracy: 1992/5104 (39.03%) Max Accuracy: 2243/5104 (43.95%) \n",
      "\n",
      "Train Epoch:17 Adversarial Loss: 0.079337\n",
      "Accuracy: 1942/5104 (38.05%) Max Accuracy: 2243/5104 (43.95%) \n",
      "\n",
      "Train Epoch:18 Adversarial Loss: 0.053679\n",
      "Accuracy: 1805/5104 (35.36%) Max Accuracy: 2243/5104 (43.95%) \n",
      "\n",
      "Train Epoch:19 Adversarial Loss: 0.066073\n",
      "Accuracy: 2290/5104 (44.87%) Max Accuracy: 2290/5104 (44.87%) \n",
      "\n",
      "Train Epoch:20 Adversarial Loss: 0.071552\n",
      "Accuracy: 1915/5104 (37.52%) Max Accuracy: 2290/5104 (44.87%) \n",
      "\n",
      "Train Epoch:21 Adversarial Loss: 0.067232\n",
      "Accuracy: 2136/5104 (41.85%) Max Accuracy: 2290/5104 (44.87%) \n",
      "\n",
      "Train Epoch:22 Adversarial Loss: 0.073985\n",
      "Accuracy: 2281/5104 (44.69%) Max Accuracy: 2290/5104 (44.87%) \n",
      "\n",
      "Train Epoch:23 Adversarial Loss: 0.046425\n",
      "Accuracy: 1961/5104 (38.42%) Max Accuracy: 2290/5104 (44.87%) \n",
      "\n",
      "Train Epoch:24 Adversarial Loss: 0.070862\n",
      "Accuracy: 2173/5104 (42.57%) Max Accuracy: 2290/5104 (44.87%) \n",
      "\n",
      "Train Epoch:25 Adversarial Loss: 0.057544\n",
      "Accuracy: 2033/5104 (39.83%) Max Accuracy: 2290/5104 (44.87%) \n",
      "\n",
      "Train Epoch:26 Adversarial Loss: 0.077775\n",
      "Accuracy: 2066/5104 (40.48%) Max Accuracy: 2290/5104 (44.87%) \n",
      "\n",
      "Train Epoch:27 Adversarial Loss: 0.065703\n",
      "Accuracy: 1995/5104 (39.09%) Max Accuracy: 2290/5104 (44.87%) \n",
      "\n",
      "Train Epoch:28 Adversarial Loss: 0.060920\n",
      "Accuracy: 2046/5104 (40.09%) Max Accuracy: 2290/5104 (44.87%) \n",
      "\n",
      "Train Epoch:29 Adversarial Loss: 0.031150\n",
      "Accuracy: 2233/5104 (43.75%) Max Accuracy: 2290/5104 (44.87%) \n",
      "\n",
      "Train Epoch:30 Adversarial Loss: 0.061511\n",
      "Accuracy: 2014/5104 (39.46%) Max Accuracy: 2290/5104 (44.87%) \n",
      "\n",
      "Train Epoch:31 Adversarial Loss: 0.056372\n",
      "Accuracy: 2044/5104 (40.05%) Max Accuracy: 2290/5104 (44.87%) \n",
      "\n",
      "Train Epoch:32 Adversarial Loss: 0.071164\n",
      "Accuracy: 2069/5104 (40.54%) Max Accuracy: 2290/5104 (44.87%) \n",
      "\n",
      "Train Epoch:33 Adversarial Loss: 0.088262\n",
      "Accuracy: 1865/5104 (36.54%) Max Accuracy: 2290/5104 (44.87%) \n",
      "\n",
      "Train Epoch:34 Adversarial Loss: 0.041667\n",
      "Accuracy: 1961/5104 (38.42%) Max Accuracy: 2290/5104 (44.87%) \n",
      "\n",
      "Train Epoch:35 Adversarial Loss: 0.040700\n",
      "Accuracy: 2087/5104 (40.89%) Max Accuracy: 2290/5104 (44.87%) \n",
      "\n",
      "Train Epoch:36 Adversarial Loss: 0.050983\n",
      "Accuracy: 2239/5104 (43.87%) Max Accuracy: 2290/5104 (44.87%) \n",
      "\n",
      "Train Epoch:37 Adversarial Loss: 0.033835\n",
      "Accuracy: 2118/5104 (41.50%) Max Accuracy: 2290/5104 (44.87%) \n",
      "\n",
      "Train Epoch:38 Adversarial Loss: 0.035101\n",
      "Accuracy: 2181/5104 (42.73%) Max Accuracy: 2290/5104 (44.87%) \n",
      "\n",
      "Train Epoch:39 Adversarial Loss: 0.051304\n",
      "Accuracy: 2115/5104 (41.44%) Max Accuracy: 2290/5104 (44.87%) \n",
      "\n",
      "Train Epoch:40 Adversarial Loss: 0.069613\n",
      "Accuracy: 2133/5104 (41.79%) Max Accuracy: 2290/5104 (44.87%) \n",
      "\n",
      "Train Epoch:41 Adversarial Loss: 0.099949\n",
      "Accuracy: 2202/5104 (43.14%) Max Accuracy: 2290/5104 (44.87%) \n",
      "\n",
      "Train Epoch:42 Adversarial Loss: 0.048544\n",
      "Accuracy: 1979/5104 (38.77%) Max Accuracy: 2290/5104 (44.87%) \n",
      "\n",
      "Train Epoch:43 Adversarial Loss: 0.076116\n",
      "Accuracy: 1939/5104 (37.99%) Max Accuracy: 2290/5104 (44.87%) \n",
      "\n",
      "Train Epoch:44 Adversarial Loss: 0.047183\n",
      "Accuracy: 1981/5104 (38.81%) Max Accuracy: 2290/5104 (44.87%) \n",
      "\n",
      "Train Epoch:45 Adversarial Loss: 0.032522\n",
      "Accuracy: 2162/5104 (42.36%) Max Accuracy: 2290/5104 (44.87%) \n",
      "\n",
      "Train Epoch:46 Adversarial Loss: 0.067633\n",
      "Accuracy: 2185/5104 (42.81%) Max Accuracy: 2290/5104 (44.87%) \n",
      "\n",
      "Train Epoch:47 Adversarial Loss: 0.047032\n",
      "Accuracy: 2297/5104 (45.00%) Max Accuracy: 2297/5104 (45.00%) \n",
      "\n",
      "Train Epoch:48 Adversarial Loss: 0.066934\n",
      "Accuracy: 2095/5104 (41.05%) Max Accuracy: 2297/5104 (45.00%) \n",
      "\n",
      "Train Epoch:49 Adversarial Loss: 0.037279\n",
      "Accuracy: 2113/5104 (41.40%) Max Accuracy: 2297/5104 (45.00%) \n",
      "\n",
      "Train Epoch:50 Adversarial Loss: 0.049830\n",
      "Accuracy: 1980/5104 (38.79%) Max Accuracy: 2297/5104 (45.00%) \n",
      "\n",
      "Train Epoch:51 Adversarial Loss: 0.030537\n",
      "Accuracy: 2267/5104 (44.42%) Max Accuracy: 2297/5104 (45.00%) \n",
      "\n",
      "Train Epoch:52 Adversarial Loss: 0.082310\n",
      "Accuracy: 2068/5104 (40.52%) Max Accuracy: 2297/5104 (45.00%) \n",
      "\n",
      "Train Epoch:53 Adversarial Loss: 0.060861\n",
      "Accuracy: 1994/5104 (39.07%) Max Accuracy: 2297/5104 (45.00%) \n",
      "\n",
      "Train Epoch:54 Adversarial Loss: 0.051765\n",
      "Accuracy: 1926/5104 (37.74%) Max Accuracy: 2297/5104 (45.00%) \n",
      "\n",
      "Train Epoch:55 Adversarial Loss: 0.039287\n",
      "Accuracy: 2166/5104 (42.44%) Max Accuracy: 2297/5104 (45.00%) \n",
      "\n",
      "Train Epoch:56 Adversarial Loss: 0.047594\n",
      "Accuracy: 1878/5104 (36.79%) Max Accuracy: 2297/5104 (45.00%) \n",
      "\n",
      "Train Epoch:57 Adversarial Loss: 0.063247\n",
      "Accuracy: 1975/5104 (38.70%) Max Accuracy: 2297/5104 (45.00%) \n",
      "\n",
      "Train Epoch:58 Adversarial Loss: 0.059806\n",
      "Accuracy: 2197/5104 (43.04%) Max Accuracy: 2297/5104 (45.00%) \n",
      "\n",
      "Train Epoch:59 Adversarial Loss: 0.050852\n",
      "Accuracy: 2256/5104 (44.20%) Max Accuracy: 2297/5104 (45.00%) \n",
      "\n",
      "Train Epoch:60 Adversarial Loss: 0.061769\n",
      "Accuracy: 2016/5104 (39.50%) Max Accuracy: 2297/5104 (45.00%) \n",
      "\n",
      "Train Epoch:61 Adversarial Loss: 0.061090\n",
      "Accuracy: 2078/5104 (40.71%) Max Accuracy: 2297/5104 (45.00%) \n",
      "\n",
      "Train Epoch:62 Adversarial Loss: 0.058498\n",
      "Accuracy: 1956/5104 (38.32%) Max Accuracy: 2297/5104 (45.00%) \n",
      "\n",
      "Train Epoch:63 Adversarial Loss: 0.054771\n",
      "Accuracy: 2189/5104 (42.89%) Max Accuracy: 2297/5104 (45.00%) \n",
      "\n",
      "Train Epoch:64 Adversarial Loss: 0.051374\n",
      "Accuracy: 2050/5104 (40.16%) Max Accuracy: 2297/5104 (45.00%) \n",
      "\n",
      "Train Epoch:65 Adversarial Loss: 0.047050\n",
      "Accuracy: 2106/5104 (41.26%) Max Accuracy: 2297/5104 (45.00%) \n",
      "\n",
      "Train Epoch:66 Adversarial Loss: 0.043333\n",
      "Accuracy: 1922/5104 (37.66%) Max Accuracy: 2297/5104 (45.00%) \n",
      "\n",
      "Train Epoch:67 Adversarial Loss: 0.053992\n",
      "Accuracy: 2082/5104 (40.79%) Max Accuracy: 2297/5104 (45.00%) \n",
      "\n",
      "Train Epoch:68 Adversarial Loss: 0.057715\n",
      "Accuracy: 2102/5104 (41.18%) Max Accuracy: 2297/5104 (45.00%) \n",
      "\n",
      "Train Epoch:69 Adversarial Loss: 0.044693\n",
      "Accuracy: 2074/5104 (40.63%) Max Accuracy: 2297/5104 (45.00%) \n",
      "\n",
      "Train Epoch:70 Adversarial Loss: 0.064977\n",
      "Accuracy: 2093/5104 (41.01%) Max Accuracy: 2297/5104 (45.00%) \n",
      "\n",
      "Train Epoch:71 Adversarial Loss: 0.058114\n",
      "Accuracy: 2033/5104 (39.83%) Max Accuracy: 2297/5104 (45.00%) \n",
      "\n",
      "Train Epoch:72 Adversarial Loss: 0.046518\n",
      "Accuracy: 2199/5104 (43.08%) Max Accuracy: 2297/5104 (45.00%) \n",
      "\n",
      "Train Epoch:73 Adversarial Loss: 0.056990\n",
      "Accuracy: 2062/5104 (40.40%) Max Accuracy: 2297/5104 (45.00%) \n",
      "\n",
      "Train Epoch:74 Adversarial Loss: 0.077286\n",
      "Accuracy: 2083/5104 (40.81%) Max Accuracy: 2297/5104 (45.00%) \n",
      "\n",
      "Train Epoch:75 Adversarial Loss: 0.044063\n",
      "Accuracy: 2086/5104 (40.87%) Max Accuracy: 2297/5104 (45.00%) \n",
      "\n",
      "Train Epoch:76 Adversarial Loss: 0.040282\n",
      "Accuracy: 1960/5104 (38.40%) Max Accuracy: 2297/5104 (45.00%) \n",
      "\n",
      "Train Epoch:77 Adversarial Loss: 0.044950\n",
      "Accuracy: 2046/5104 (40.09%) Max Accuracy: 2297/5104 (45.00%) \n",
      "\n",
      "Train Epoch:78 Adversarial Loss: 0.036888\n",
      "Accuracy: 2397/5104 (46.96%) Max Accuracy: 2397/5104 (46.96%) \n",
      "\n",
      "Train Epoch:79 Adversarial Loss: 0.037962\n",
      "Accuracy: 2098/5104 (41.11%) Max Accuracy: 2397/5104 (46.96%) \n",
      "\n",
      "Train Epoch:80 Adversarial Loss: 0.057497\n",
      "Accuracy: 2238/5104 (43.85%) Max Accuracy: 2397/5104 (46.96%) \n",
      "\n",
      "Train Epoch:81 Adversarial Loss: 0.072930\n",
      "Accuracy: 2099/5104 (41.12%) Max Accuracy: 2397/5104 (46.96%) \n",
      "\n",
      "Train Epoch:82 Adversarial Loss: 0.030100\n",
      "Accuracy: 2181/5104 (42.73%) Max Accuracy: 2397/5104 (46.96%) \n",
      "\n",
      "Train Epoch:83 Adversarial Loss: 0.041911\n",
      "Accuracy: 2177/5104 (42.65%) Max Accuracy: 2397/5104 (46.96%) \n",
      "\n",
      "Train Epoch:84 Adversarial Loss: 0.049960\n",
      "Accuracy: 2064/5104 (40.44%) Max Accuracy: 2397/5104 (46.96%) \n",
      "\n",
      "Train Epoch:85 Adversarial Loss: 0.055402\n",
      "Accuracy: 2266/5104 (44.40%) Max Accuracy: 2397/5104 (46.96%) \n",
      "\n",
      "Train Epoch:86 Adversarial Loss: 0.045283\n",
      "Accuracy: 2177/5104 (42.65%) Max Accuracy: 2397/5104 (46.96%) \n",
      "\n",
      "Train Epoch:87 Adversarial Loss: 0.055085\n",
      "Accuracy: 2254/5104 (44.16%) Max Accuracy: 2397/5104 (46.96%) \n",
      "\n",
      "Train Epoch:88 Adversarial Loss: 0.057034\n",
      "Accuracy: 2060/5104 (40.36%) Max Accuracy: 2397/5104 (46.96%) \n",
      "\n",
      "Train Epoch:89 Adversarial Loss: 0.053382\n",
      "Accuracy: 2105/5104 (41.24%) Max Accuracy: 2397/5104 (46.96%) \n",
      "\n",
      "Train Epoch:90 Adversarial Loss: 0.048665\n",
      "Accuracy: 2004/5104 (39.26%) Max Accuracy: 2397/5104 (46.96%) \n",
      "\n",
      "Train Epoch:91 Adversarial Loss: 0.082075\n",
      "Accuracy: 2316/5104 (45.38%) Max Accuracy: 2397/5104 (46.96%) \n",
      "\n",
      "Train Epoch:92 Adversarial Loss: 0.064706\n",
      "Accuracy: 2238/5104 (43.85%) Max Accuracy: 2397/5104 (46.96%) \n",
      "\n",
      "Train Epoch:93 Adversarial Loss: 0.082407\n",
      "Accuracy: 2154/5104 (42.20%) Max Accuracy: 2397/5104 (46.96%) \n",
      "\n",
      "Train Epoch:94 Adversarial Loss: 0.041934\n",
      "Accuracy: 2308/5104 (45.22%) Max Accuracy: 2397/5104 (46.96%) \n",
      "\n",
      "Train Epoch:95 Adversarial Loss: 0.067258\n",
      "Accuracy: 2267/5104 (44.42%) Max Accuracy: 2397/5104 (46.96%) \n",
      "\n",
      "Train Epoch:96 Adversarial Loss: 0.038158\n",
      "Accuracy: 2278/5104 (44.63%) Max Accuracy: 2397/5104 (46.96%) \n",
      "\n",
      "Train Epoch:97 Adversarial Loss: 0.043120\n",
      "Accuracy: 2264/5104 (44.36%) Max Accuracy: 2397/5104 (46.96%) \n",
      "\n",
      "Train Epoch:98 Adversarial Loss: 0.043077\n",
      "Accuracy: 2173/5104 (42.57%) Max Accuracy: 2397/5104 (46.96%) \n",
      "\n",
      "Train Epoch:99 Adversarial Loss: 0.044266\n",
      "Accuracy: 2387/5104 (46.77%) Max Accuracy: 2397/5104 (46.96%) \n",
      "\n",
      "Train Epoch:100 Adversarial Loss: 0.081314\n",
      "Accuracy: 2118/5104 (41.50%) Max Accuracy: 2397/5104 (46.96%) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cdada_model = CDADA(pkl_file_path='C:/Users/Soumya Taneja/Desktop/Sideproject/WM811Kdata/LSWMD.pkl', num_class=8)\n",
    "cdada_model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
